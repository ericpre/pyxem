{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Loading MRC files (and other binary files)\n\nThis is a simple example of how to load MRC files using Pyxem. The MRC file format is\na common format for electron microscopy data.  It is a binary format that is used\nfor storing 3D data, such as electron tomography but because it is a fairly simple format, it has\nbeen adopted in some cases to store 4D STEM data as well.\n\nFirst we will download a sample MRC file from the Pyxem data repository. This is a good way to host\ndata if you want to share it with others.  I love putting small versions (up to 50 GB) of every dataset I publish\non Zenodo and then using pooch to automate the download/extraction process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport zipfile\nimport pooch\n\ncurrent_directory = os.getcwd()\nfile_path = pooch.retrieve(\n    # URL to one of Pooch's test files\n    url=\"https://zenodo.org/records/15490547/files/ZrNbMrc.zip\",\n    known_hash=\"md5:eeac29aee5622972daa86a394a8c1d5c\",\n    progressbar=True,\n    path=current_directory,\n)\n# Unzip the file\nwith zipfile.ZipFile(file_path, \"r\") as zip_ref:\n    zip_ref.extractall(current_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading the MRC file\nWe can now load the file using the ``load`` method from hyperspy.  This method uses\nthe [MRC Reader](https://hyperspy.org/rosettasciio/supported_formats/mrc.html#mrc-format)\nto read the file. In this case, because the file was collected with a Direct Electron camera,\nthe metadata is automatically loaded as well.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import hyperspy.api as hs\n\nsignal = hs.load(\n    \"ZrNbMrc/20241021_00405_movie.mrc\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading Lazily\nIn this case the file was loaded using the numpy.memmap function,\nthis won't load the entire file into memory, but if for example you\ndo ``signal.sum()`` now the entire file will be loaded into memory.\nIn most cases it is better to just use the ``lazy=True`` option to load the file lazily.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "signal = hs.load(\"ZrNbMrc/20241021_00405_movie.mrc\", lazy=True)\n\nsignal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Controlling the Chunk Size\nThe chunk size is the number of frames that will be loaded into memory at once when\nthe signal is lazy loaded.  This can be controlled using the ``chunks`` parameter.\nA good place to start is to use the ``auto`` option for the first two dimensions, which will\nautomatically determine the chunk size based on the available memory. The last two dimensions\nare the reciprocal space dimensions, as we usually ``map`` over those dimensions we can set them\nto ``-1`` to indicate that we want to load all the data in those dimensions at once.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "signal = hs.load(\"ZrNbMrc/20241021_00405_movie.mrc\", lazy=True, chunks=(10, 10, -1, -1))\n\nsignal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Slicing the Signal\nInterestingly, binary files are sometimes faster than compressed formats.  With compressed file formats,\nlike HDF5 or Zarr, you need to decompress the entire chunk before you can access and part of the\ndata. For things like Virtual Images or slicing a signal this can add overhead.  With binary files,\nbecause the underlying data is a memory map, even for dask arrays, you can very efficiently slice parts\nof the data without loading the entire chunk into memory.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "slice_sum = signal.isig[0:10, 0:10].sum()\nslice_sum.compute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case this is faster than the compressed equivalent, because we don't have to load the\nentire chunk into memory just to throw most of it away.\n\nA couple of more things to note.  Performance of binary files is usually better on SSDs than on HDDs,\nbecause the seek time is much lower on SSDs.  This means that you can have arbitrary dask chunks and it will\nstill be fast. On HDDs, you want to keep data that is close together in the same chunk. Usually this means\nyou want chunks like (1, \"auto\", -1, -1).  This is not terribly noticeable for 1-2 GB files, somewhat noticeable\nfor 10-20 GB files, and extremely important for 100+ GB files on an HDD.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}